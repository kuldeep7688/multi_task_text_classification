{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import Trainer, TrainerCallback, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, BertConfig, BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/experiment_2024-06-20_19-47-07',\n",
       " './logs/experiement_2024-06-20_19-47-07')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 512\n",
    "NUM_TRAIN_EPOCHS = 10\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "RESULTS_DIRECTORY = './results/experiment_{}'.format(timestamp)\n",
    "\n",
    "LOGGING_DIRECTORY = './logs/experiement_{}'.format(timestamp)\n",
    "\n",
    "RESULTS_DIRECTORY, LOGGING_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCT_LABELS = [\n",
    "    'Vehicle loan or lease', 'Student loan', 'Consumer Loan'\n",
    "]\n",
    "\n",
    "ISSUE_LABELS = [\n",
    "    'Managing the loan or lease', 'Problems at the end of the loan or lease', 'Struggling to pay your loan', 'Getting a loan or lease', 'Dealing with your lender or servicer', 'Incorrect information on your report', 'Problems when you are unable to pay', 'Taking out the loan or lease'\n",
    "]\n",
    "\n",
    "PRODUCT_LABELS_TO_IDX = {\n",
    "    i : idx\n",
    "    for idx, i in enumerate(PRODUCT_LABELS)\n",
    "}\n",
    "\n",
    "IDX_TO_PRODUCT_LABELS = {\n",
    "    i: j\n",
    "    for j, i in PRODUCT_LABELS_TO_IDX.items()\n",
    "}\n",
    "\n",
    "ISSUE_LABELS_TO_IDX = {\n",
    "    i : idx\n",
    "    for idx, i in enumerate(ISSUE_LABELS)\n",
    "}\n",
    "\n",
    "IDX_TO_ISSUE_LABELS = {\n",
    "    i: j\n",
    "    for j, i in ISSUE_LABELS_TO_IDX.items()\n",
    "}\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "PRETRAINED_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_TRAIN_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function\n",
    "def tokenize(example, tokenizer):\n",
    "    return tokenizer(\n",
    "        example['consumer_complaint'], padding=True, truncation=True\n",
    "    )\n",
    "\n",
    "\n",
    "# label encoding\n",
    "def label_encoder(example):\n",
    "    example['product_label'] = PRODUCT_LABELS_TO_IDX[example['product']]     \n",
    "    example['issue_label'] = ISSUE_LABELS_TO_IDX[example['issue']]\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# initializing Config and Tokenizer\n",
    "BERT_CONFIG = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['product', 'issue', 'consumer_complaint', '__index_level_0__'],\n",
       "        num_rows: 6800\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['product', 'issue', 'consumer_complaint', '__index_level_0__'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['product', 'issue', 'consumer_complaint', '__index_level_0__'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data\n",
    "data = datasets.load_from_disk('data/complaints_dataset_obj')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /run/media/kuldeepsingh/Work/college_stuff/github/multi_task_text_classification/data/complaints_dataset_obj/train/cache-b9cfa2472282e060.arrow\n",
      "Loading cached processed dataset at /run/media/kuldeepsingh/Work/college_stuff/github/multi_task_text_classification/data/complaints_dataset_obj/val/cache-4cfff64b2cdcac2a.arrow\n",
      "Loading cached processed dataset at /run/media/kuldeepsingh/Work/college_stuff/github/multi_task_text_classification/data/complaints_dataset_obj/test/cache-47425c01f75b9f93.arrow\n",
      "Loading cached processed dataset at /run/media/kuldeepsingh/Work/college_stuff/github/multi_task_text_classification/data/complaints_dataset_obj/train/cache-4f752077556c36c9.arrow\n",
      "Loading cached processed dataset at /run/media/kuldeepsingh/Work/college_stuff/github/multi_task_text_classification/data/complaints_dataset_obj/val/cache-bee9ca709a15e723.arrow\n",
      "Loading cached processed dataset at /run/media/kuldeepsingh/Work/college_stuff/github/multi_task_text_classification/data/complaints_dataset_obj/test/cache-f12a4a703133b2be.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['product', 'issue', 'consumer_complaint', '__index_level_0__', 'input_ids', 'attention_mask', 'product_label', 'issue_label'],\n",
       "        num_rows: 6800\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['product', 'issue', 'consumer_complaint', '__index_level_0__', 'input_ids', 'attention_mask', 'product_label', 'issue_label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['product', 'issue', 'consumer_complaint', '__index_level_0__', 'input_ids', 'attention_mask', 'product_label', 'issue_label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda x: tokenize(x, tokenizer=TOKENIZER))\n",
    "data = data.map(lambda x: label_encoder(x))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the metrics \n",
    "# we will be using F1 scores for both product and issues \n",
    "# the best model will be decided using the f1 score for the product label (can be done using a weighted sum also)\n",
    "PRODUCT_METRIC = evaluate.load(\"f1\")\n",
    "ISSUE_METRIC = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    all_logits, all_labels = eval_pred\n",
    "    product_logits, issue_logits = all_logits \n",
    "    product_labels, issue_labels = all_labels\n",
    "\n",
    "    product_predictions = np.argmax(product_logits, axis=-1)\n",
    "    issue_predictions = np.argmax(issue_logits, axis=-1)\n",
    "    \n",
    "    product_computed_metrics = PRODUCT_METRIC.compute(predictions=product_predictions, references=product_labels, average='weighted')\n",
    "    issue_computed_metrics = ISSUE_METRIC.compute(predictions=issue_predictions, references=issue_labels, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'f1_product': product_computed_metrics['f1'],\n",
    "        'f1_issue': issue_computed_metrics['f1'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MultiTaskSentencePrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_product_labels, num_issue_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_product_labels = num_product_labels\n",
    "        self.num_issue_labels = num_issue_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        self.product_classifier = nn.Linear(config.hidden_size, num_product_labels)\n",
    "        self.issue_classifier = nn.Linear(config.hidden_size, num_issue_labels)\n",
    "\n",
    "        classifier_dropout = config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        \n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self, input_ids, attention_mask=None, token_type_ids=None, \n",
    "            product_label=None, issue_label=None \n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_outputs = self.dropout(outputs[1])\n",
    "        \n",
    "        logits_product = self.product_classifier(pooled_outputs)\n",
    "        logits_issue = self.issue_classifier(pooled_outputs)\n",
    "\n",
    "        loss = None\n",
    "        if product_label != None and issue_label != None:\n",
    "            loss_fct1 = nn.CrossEntropyLoss()\n",
    "            loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            loss = loss_fct1(\n",
    "                logits_product.view(-1, self.num_product_labels),\n",
    "                product_label.view(-1)\n",
    "            ) + loss_fct2(\n",
    "                logits_issue.view(-1, self.num_issue_labels),\n",
    "                issue_label.view(-1)\n",
    "            )\n",
    "        \n",
    "        return (loss, logits_product, logits_issue) if loss is not None else (logits_product, logits_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch, padding_token_id=TOKENIZER.pad_token_id):\n",
    "    input_ids = [item[\"input_ids\"][:MAX_SEQUENCE_LENGTH] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"][:MAX_SEQUENCE_LENGTH] for item in batch]\n",
    "    # token_type_ids = [item['token_type_ids'][:MAX_SEQUENCE_LENGTH] for item in batch]\n",
    "    product_label = [item[\"product_label\"] for item in batch]\n",
    "    issue_label = [item[\"issue_label\"] for item in batch]\n",
    "\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "    input_ids = torch.tensor([ids + [padding_token_id] * (max_len - len(ids)) for ids in input_ids])\n",
    "    attention_masks = torch.tensor([masks + [padding_token_id] * (max_len - len(masks)) for masks in attention_masks])\n",
    "    # token_type_ids = torch.tensor([ids + [padding_token_id] * (max_len - len(ids)) for ids in token_type_ids])\n",
    "    product_label = torch.tensor([i for i in product_label])\n",
    "    issue_label = torch.tensor([i for i in issue_label])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": attention_masks, \n",
    "        # 'token_type_ids': token_type_ids,\n",
    "        \"product_label\": product_label, \n",
    "        'issue_label': issue_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiTaskSentencePrediction were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'issue_classifier.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.bias', 'product_classifier.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'issue_classifier.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'product_classifier.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskSentencePrediction.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME,\n",
    "    config=BERT_CONFIG,\n",
    "    num_product_labels=len(PRODUCT_LABELS), num_issue_labels=len(ISSUE_LABELS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = RESULTS_DIRECTORY\n",
    "evaluation_strategy = 'epoch'\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradint_accumulation_steps = 2\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1\n",
    "num_train_epochs = NUM_TRAIN_EPOCHS\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.05\n",
    "logging_dir = LOGGING_DIRECTORY\n",
    "logging_strategy = 'epoch'\n",
    "save_strategy = 'epoch'\n",
    "save_total_limit = 1\n",
    "label_names = ['product_label', 'issue_label']\n",
    "load_best_model_at_end = True\n",
    "metric_for_best_model = 'eval_f1_product'\n",
    "greater_is_better = True\n",
    "label_smoothing_factor = 0\n",
    "report_to = 'tensorboard'\n",
    "gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir=logging_dir,\n",
    "    label_names=label_names,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    logging_strategy=logging_strategy,\n",
    "    save_strategy=save_strategy,\n",
    "    save_total_limit=save_total_limit,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    "    greater_is_better=greater_is_better,\n",
    "    label_smoothing_factor=label_smoothing_factor,\n",
    "    report_to=report_to,\n",
    "    gradient_checkpointing=gradient_checkpointing\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStoppingCallback(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuldeepsingh/anaconda3/envs/pytorch2/lib/python3.9/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test'],\n",
    "    # tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stop_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d40dd6192b4fccb44df51db941781c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5144, 'learning_rate': 1.894736842105263e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d60ca169b042158b6ad46e4dc04959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.050814390182495, 'eval_f1_product': 0.7330244035915434, 'eval_f1_issue': 0.381744481523802, 'eval_runtime': 29.7564, 'eval_samples_per_second': 67.212, 'eval_steps_per_second': 16.803, 'epoch': 1.0}\n",
      "{'loss': 1.8145, 'learning_rate': 1.6842105263157896e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee0a69b82524b34a3a517f128eda4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.725622534751892, 'eval_f1_product': 0.7874698075495403, 'eval_f1_issue': 0.4062835622465695, 'eval_runtime': 27.0552, 'eval_samples_per_second': 73.923, 'eval_steps_per_second': 18.481, 'epoch': 2.0}\n",
      "{'loss': 1.6667, 'learning_rate': 1.4736842105263159e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fd400677b949aba08d33551f0d96bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8767348527908325, 'eval_f1_product': 0.8008055543134794, 'eval_f1_issue': 0.43789558345102136, 'eval_runtime': 27.5807, 'eval_samples_per_second': 72.515, 'eval_steps_per_second': 18.129, 'epoch': 3.0}\n",
      "{'loss': 1.5752, 'learning_rate': 1.263157894736842e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11f9411ab024cbeb3ef4cfa62c558c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7505781650543213, 'eval_f1_product': 0.8061989367601342, 'eval_f1_issue': 0.46253332276111997, 'eval_runtime': 27.511, 'eval_samples_per_second': 72.698, 'eval_steps_per_second': 18.175, 'epoch': 4.0}\n",
      "{'loss': 1.4193, 'learning_rate': 1.0526315789473684e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0416ae5b347848908ca268c84642c64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7924469709396362, 'eval_f1_product': 0.8220689915797993, 'eval_f1_issue': 0.49385480692359296, 'eval_runtime': 27.8452, 'eval_samples_per_second': 71.826, 'eval_steps_per_second': 17.956, 'epoch': 5.0}\n",
      "{'loss': 1.2805, 'learning_rate': 8.421052631578948e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373cbf7866184deaa2503fae543a9de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7341166734695435, 'eval_f1_product': 0.8340649212035753, 'eval_f1_issue': 0.517574158893834, 'eval_runtime': 28.0977, 'eval_samples_per_second': 71.18, 'eval_steps_per_second': 17.795, 'epoch': 6.0}\n",
      "{'loss': 1.1544, 'learning_rate': 6.31578947368421e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3569a5f32fa4b0d93dc104fbc74c82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.811495065689087, 'eval_f1_product': 0.8416251659607228, 'eval_f1_issue': 0.5500737803473196, 'eval_runtime': 27.3928, 'eval_samples_per_second': 73.012, 'eval_steps_per_second': 18.253, 'epoch': 7.0}\n",
      "{'loss': 1.0247, 'learning_rate': 4.210526315789474e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a798eb60f444f1981e8217b9c19a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9135061502456665, 'eval_f1_product': 0.8441983795419775, 'eval_f1_issue': 0.5612920205131465, 'eval_runtime': 27.6158, 'eval_samples_per_second': 72.422, 'eval_steps_per_second': 18.106, 'epoch': 8.0}\n",
      "{'loss': 0.923, 'learning_rate': 2.105263157894737e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cde33e6d80e41e1b40af0c4bc58fbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9395087957382202, 'eval_f1_product': 0.8449531285266052, 'eval_f1_issue': 0.5646385661481008, 'eval_runtime': 27.8557, 'eval_samples_per_second': 71.799, 'eval_steps_per_second': 17.95, 'epoch': 9.0}\n",
      "{'loss': 0.8313, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4d2358ec334f96b9237fa70fc72771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9870702028274536, 'eval_f1_product': 0.8420717800675612, 'eval_f1_issue': 0.5642200057911417, 'eval_runtime': 27.6061, 'eval_samples_per_second': 72.448, 'eval_steps_per_second': 18.112, 'epoch': 10.0}\n",
      "{'train_runtime': 3656.3222, 'train_samples_per_second': 18.598, 'train_steps_per_second': 4.649, 'train_loss': 1.420393834731158, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17000, training_loss=1.420393834731158, metrics={'train_runtime': 3656.3222, 'train_samples_per_second': 18.598, 'train_steps_per_second': 4.649, 'train_loss': 1.420393834731158, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e27309840624c439dc5eba20258c9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9155381917953491,\n",
       " 'eval_f1_product': 0.8350117423827834,\n",
       " 'eval_f1_issue': 0.5478804429185374,\n",
       " 'eval_runtime': 25.7507,\n",
       " 'eval_samples_per_second': 77.668,\n",
       " 'eval_steps_per_second': 19.417,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
